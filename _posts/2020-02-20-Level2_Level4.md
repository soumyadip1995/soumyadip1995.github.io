# Level 2 vs Level 4 Autonomous Driving Systems (Featuring:- Tesla,Waymo):- Level 2 and the Tesla Autopilot Part 1

![alt text](https://www.skynettoday.com/assets/img/editorials/autonomous_vehicles/self_driving_simulator.gif)

*Footage of a Tesla car autonomously driving around along with the sensing and perception involved. [Source](https://www.skynettoday.com/editorials/autonomous_vehicles)*

## **Table of Contents**

1. TOC
{:toc}


> - Updates:- Feb 20,2020.
> - Updates :- Paper 2, Tesla Autopilot, Feb 21,2020.
> - Updates:- Tesla Autopilot, Feb 22,2020.
> - Updates:- Paper Citations, Feb 23,2020.

## **A brief introduction to level 2 Autonomous driving systems**

> *â€œThe fundamental message consumers should be taking today is that itâ€™s financially insane to buy anything other than a Tesla...
Itâ€™ll be like owning a horse in three years. I mean, fine if you want to own a horse, but you should go into it with that expectation.â€- Elon Musk, Tesla Autonomy Day*

### **The Aim**

The aim through this series of blog posts is to give the readers a brief insight into level 2 and level 4 autonomous driving systems. We will try to gain perspective on what constitutes as level 2 and level 4 along with examples and the work that has been done so far on these two levels by the likes of companies like Tesla and Waymo. 

A study into the data driven aspect of self driving as well as the holistic approach should definitely provide clarity. An overview into the data driven, modular and integrated systems have been provided. Some open queries like perception (detection) and balancing enjoyability will be looked into. Vision techniques that cover Level 2 have been discussed along with an overview of the Tesla Autopilot. 

This is only the First part of a series of posts.

[Github Repository](https://github.com/soumyadip1995/Autonomous-Driving-System)

### **Levels of Driving Automation**


Lets talk about the classification levels first. Automated driving systems can be classified into six 
levels â€“ ranging from fully manual to fully automated systems â€“ was published in 2014 by SAE International.  â€“ ranging from fully manual to fully automated systems â€“ 
was published in 2014 by SAE International. 

[*SAE International, previously known as the Society of Automotive Engineers, is a U.S.-based, globally active professional association and standards developing organization for engineering professionals in various industries. Principal emphasis is placed on global transport industries such as aerospace, automotive, and commercial vehicles.*]

This classification is based on the amount of driver intervention and attentiveness required, rather than the vehicle's capabilities, although these are loosely related.

In SAE's automation level definitions, "driving mode" means "a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)"

- Level 0: The automated system issues warnings and may momentarily intervene but has no sustained vehicle control.

- Level 1: ("hands on"): The driver and the automated system share control of the vehicle. Examples are systems where the driver controls steering and the automated system controls engine power to maintain a set speed (Cruise Control) or engine and brake power to maintain and vary speed (Adaptive Cruise Control or ACC); and Parking Assistance, where steering is automated while speed is under manual control. The driver must be ready to retake full control at any time. 

- Level 2: ("hands off"): The automated system takes full control of the vehicle: accelerating, braking, and steering. The driver must monitor the driving and be prepared to intervene immediately at any time if the automated system fails to respond properly. 

- Level 3: ("eyes off"): The driver can safely turn their attention away from the driving tasks, e.g. the driver can text or watch a movie. The vehicle will handle situations that call for an immediate response, like emergency braking. The driver must still be prepared to intervene within some limited time, when called upon by the vehicle to do so.

- Level 4: ("mind off"): As level 3, but no driver attention is ever required for safety, e.g. the driver may safely go to sleep or leave the driver's seat. Self-driving is supported only in limited spatial areas (geofenced) or under special circumstances. Outside of these areas or circumstances, the vehicle must be able to safely abort the trip, e.g. park the car, if the driver does not retake control.

- Level 5: ("steering wheel optional"): No human intervention is required at all. An example would be a robotic taxi.

*Source:- Wikipedia*

## **Level 2**
 
 As mentioned in the above definition, the automated system takes full control of the vehicle. But, the driver, still needs to be prepared to intervene if the system fails. 

One of the recent examples of a Level 2 Autonomous driving system comes from Tesla. Yes, you guessed it right-  The **Tesla Autopilot.** 

> "*Autopilot is a good thing to have in planes, and we should have it in cars."- Elon Musk, 2013.*

The Tesla Autopilot is an [advanced driver-assistance system](https://en.wikipedia.org/wiki/Advanced_driver-assistance_systems) feature offered by Tesla that has 

- **Lane centering/Auto Steer**- In road-transport terminology, lane centering, also known as auto steer, is a mechanism designed to keep a car centered in the lane, relieving the driver of the task of steering.

- **Adaptive cruise control**- Adaptive cruise control (ACC) is an available cruise control system for road vehicles that automatically adjusts the vehicle speed to maintain a safe distance from vehicles ahead .

- **Self-parking**- Automatic parking is an autonomous car-maneuvering system that moves a vehicle from a traffic lane into a parking spot to perform parallel, perpendicular, or angle parking. The automatic parking system aims to enhance the comfort and safety of driving in constrained environments where much attention and experience is required to steer the car.

![alt_text](https://1.bp.blogspot.com/-uLmXT_e_ox4/XlE_ZpTE-UI/AAAAAAAAKag/5aTBjxjB3BwbFW5GuhKoC_pvFbmUolTsQCNcBGAsYHQ/s1600/Screenshot%2B%2528297%2529.png)

*Tesla's currently estimated Autopilot Miles is around 2.2 Billion and is Projected to Increase to 4 Billion by 2021. A Screengrab from Lex Fridman's Lecture, Deep Learning SOTA-2020* 



In all of these features, the driver is responsible and the car requires constant supervision. 
As an upgrade to the base Autopilot capabilities, the company's stated intent is to offer **Full self-driving (FSD)** .
More on FSD Later on.


### **Deep Learning**

Level 2 takes control over acceleration, braking , steering. 
So, a part of the topic needs to cover the Deep Learning side as well as the Hardware side of the story
in order to look into how Level 2 can be acheived. We'll look into the Tesla Autopilot story a little later.
But first lets see how a level 2 state can be created and put into use. 

### **Using Computer Vision Techniques**

Let us assume for a moment, a scenario where Deep Learning is the cake to achieve Level 2.
In this case,  Deep Learning is used along with external hardware like UV sensors, cameras etc.
Neural Networks are trained and retrained using datasets [datasets consisting of different environments and edge cases] 
at scale, in some cases networks that improve over time (will be discussed later on) and deployed. 

![alt_text](https://1.bp.blogspot.com/-uk3DP3Tz8h4/XlE_YDgXdeI/AAAAAAAAKac/WqRHrYHVmNwXVmapUfAXY90eXhQVhU7awCNcBGAsYHQ/s1600/Screenshot%2B%2528296%2529.png)

*A Comparison Between Level 2 and Level 4 (Vision vs Lidar). A ScreenGrab from Lex Fridman's Lecture, Deep Learning SOTA- 2020*

This is the kind of Deep Learning that is supervised as it easy to collect data and train the network.
There is a good chance that this information will be of the highest resolution.
The cons of such a system is that it will require a huge amount of data and is not explainable enough. 
Let us look at a few examples:-

### **Examples**

#### **Paper 1:- [End to End learning for Self Driving Cars](https://arxiv.org/pdf/1604.07316.pdf)**

In 2016, researchers from NVIDIA trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved to be powerful. In the [paper](https://arxiv.org/pdf/1604.07316.pdf), the authors were able to demonstrate that CNNs were able to learn the entire task of lane and road following without manual decomposition into road or lane marking detection, path planning, and control. It learns the entire processing pipeline needed to steer an automobile.

>"*A small amount of training data from less than a hundred hours of driving was sufficient to train the car to operate in diverse conditions, on highways, local and residential
roads in sunny, cloudy, and rainy conditions. The CNN is able to learn meaningful road features from a very sparse training signal (steering alone)"- Conclusion from End to End Learning for Self-Driving Cars.*

#### **Methodology**

A Defense Advanced Research Projects Agency (DARPA) seedling project known as DARPA Autonomous Vehicle (DAVE) was used. DAVE-2, the system that was built in the paper on top of DAVE.

To get the training data for DAVE-2 ,a data acquisition car was used. The car was equipped with three cameras mounted behind the windshield of the car. Video was captured simultaneously with the steering angle data.  

Images were fed into a CNN which
then calculated a proposed NN steering command. 
The proposed command was then compared to the desired command for that image and the weights of the CNN
were adjusted to bring the CNN output closer to
the desired output. The weight adjustment is accomplished using back propagation. 

![alt text](https://miro.medium.com/max/1184/1*bO9_3Vp5InMkdJ3_5b4D1Q.png)

*A block diagram of the training system. Image from the End to End Learning for Self driving cars paper.*


**Dataset**- Most road data was collected in central New Jersey,  highway data was also collected from Illinois, Michigan, Pennsylvania, and New York. 

In contrast to modular engineering for self driving, this provides a more holistic view that is data driven.


#### **Paper 2:- [LaneNet: Real-Time Lane Detection Networks for Autonomous Driving](https://arxiv.org/pdf/1807.01726.pdf)**

[ZE Wang et.al,2018](https://arxiv.org/pdf/1807.01726.pdf). When we talk about applying in autonomous driving projects, lane detection is one of the most preliminary project ideas that come to mind for a self driving car enthusiast. Since, we are considering Vision techiques for a level 2 system, we are going to take a look into methods that can provide some perspective where Deep Learning is the cake. This is a more modular approach to self driving. In this example, we are going to look into LaneNet. 

#### **Methodology**

The authors of the paper has proposed a deep neural network based method called LaneNet, in order to break down the lane detection into two stages:-

> "lane edge proposal and lane line localization. Stage one uses a lane edge proposal network for
pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based
on lane edge proposals....Despite all the difficulties, our lane detection is shown to be
robust to both highway and urban road scenarios method
without relying on any assumptions on the lane number or
the lane line patterns. The high running speed and low computational cost endow our LaneNet the capability of being
deployed on vehicle-based systems. Experiments validate
that our LaneNet consistently delivers outstanding performances on real world traffic scenarios.
"- from the Abstract from [ZE Wang et.al,2018](https://arxiv.org/pdf/1807.01726.pdf)


In order to work towards a more effective generalized, low computation cost, and real-time vehicle-based solution, the authors have proposed LaneNet, a deep Neural network based system that breaks down lane detection into two parts:- Lane edge proposal and lane Localization.

> " In the lane edge proposal stage, the proposal network runs binary classification on every pixel of an input
image for generating lane edge proposals, which are served
as the input to the lane line localization network in the second stage."- from the Introduction from [ZE Wang et.al,2018](https://arxiv.org/pdf/1807.01726.pdf)

The first  network detects the
edges of lane marks, and generates a pixel-wise lane edge proposal map. The second localization network determines
the localization and shape of each lane based on lane edge
proposals. The authors have adopted a light weight Encoder Decoder architecture that uses convolutional layers and deconv layers.

> "The encoder takes an IPM image of the front
view of a vehicle as the input, and hierarchically extracts
the features. The decoder progressively recovers the resolution of the feature map and produce a pixel-wise lane edge
proposal map."- from the paper section 2

*IPM - Inverse Perspective Mapping-  The Inverse Perspective Mapping (IPM) The angle of view under which a scene is acquired and the distance of the objects from the camera (namely the perspective effect) contribute to associate a different information content to each pixel of an image.*

![alt_text](https://1.bp.blogspot.com/-Coj1Qerppz4/Xk-cI7xV9kI/AAAAAAAAKZU/vDuJUsmKI5ovmNEfsnrH-vmrE_-bXzIJACNcBGAsYHQ/s1600/Screenshot%2B%2528293%2529.png)

*IPM image from the front view of a vehicle- Image from the paper*

The line localization network also uses an encoding-decoder
structure. It takes the coordinates of the lane edges as input,
and applies a series of 1D convolution and pooling operations
to encode an input to a low dimensional representation. An (LSTM) based
decoder is used to  decode the representation of each lane in the image. 

![alt_text](https://1.bp.blogspot.com/-ATyfXI7xIqI/Xk-cHCXxSUI/AAAAAAAAKZQ/bsKC_uT_1qEP1-6tCar4nht1xrhfTswxACNcBGAsYHQ/s1600/Screenshot%2B%2528290%2529.png)

*Overall Architecture:- (a) Lane Edge Proposal Network and (b) Lane Line localization Network- Image from the paper*

**Training and Testing**- In the paper, the model takes an image of
the front view of a vehicle as input, and outputs a lane edge
probability map of the same size as the input image. It happens in a supervised manner. LaneNet has been tested on real world traffic data. The authors had compared their method with other lane detection
methods. The dataset consists of more than 5000 annotated front view images taken on both highways and urban roads. 

![alt text](https://d3i71xaburhd42.cloudfront.net/52a8866dfd2bce6a1169eba1a47ad2008ef4eecd/7-Figure4-1.png)

*The original images along with the corresponding lane edge proposal maps and final detection results- Image from the paper*

LaneNet can be applied to diverse situations. Since Deep Learning here is the cake, a high robustness is expected. The two stage detection pipeline put forth by the authors, greatly reduces computational cost. 

**Other Papers**- 
- [Real time Lane Detection for Autonomous Vehicles](https://ieeexplore.ieee.org/document/4580573)
- [Towards End-to-End Lane Detection: an Instance Segmentation
Approach](https://arxiv.org/pdf/1802.05591v1.pdf)
- [Lane Detection in Low-light Conditions Using an Efficient Data
Enhancement : Light Conditions Style Transfer](https://arxiv.org/pdf/2002.01177.pdf)

**Github Repositories**
- [Robust Lane Detection and Tracking](https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking)

### **The Tesla AutoPilot**

![alt_text](https://article.images.consumerreports.org/f_auto/prod/content/dam/CRO%20Images%202018/Cars/June/CR-Cars-InlineHero-Tesla-Model-3-Hands-off-6-18)

Tesla Autopilot is an advanced driver-assistance system feature offered by Tesla that has lane centering, adaptive cruise control, self-parking, the ability to automatically change lanes, navigate autonomously on limited access freeways, and the ability to summon the car to and from a garage or parking spot. In all of these features, the driver is responsible and the car requires constant supervision.

In this section, I am going to briefly try and explain the overview of the Tesla Autopilot.

#### **A Brief Overview of Tesla's Autopilot Mode**

So, the question is how does Tesla's Autopilot work. The answer is humans. According to Fred Lambert from Electrek, Musk referred to the first round of Model S owners with Autopilot as "expert trainers" that will collect and deliver tons of important data for Tesla's intelligence network. 

When Musk was asked how Autopilot on the Model S is different from similar cruise control and lane-keeping systems from Audi and Mercedes,this was his answer:

> "When one car learns something, the whole fleet learns it"

Musk had revealed in a press conference that whenever a model S owner drives a vehicle on autopilot, it feeds a collective network. In other words, each tesla owner trains a network whenever they are driving the vehicle on Autopilot. This method discourages unwanted driving behavior. All Teslas learn together..!!. It builds a huge training data, so all Teslas can learn very quicky. 

According to Elon Musk there are Four Pillars for Autonomous driving:-

- The first one is a long distance radar system that can see through virtually anything, this means snow, dust , etc.
- The second is cameras with Image Recognition.
- The third is Ultrasonic Sensors.
- The fourth is Satellite Imagery with real time Traffic.

When these systems are all integrated, we get a robust system for the vehicle. Check out [this](https://www.youtube.com/watch?v=xxM5JUJFWV4) video by cnet.

[![alt_text](https://i.ytimg.com/vi/IkSw2SZQENU/maxresdefault.jpg)](https://www.youtube.com/watch?v=IkSw2SZQENU)

*Watch the above video to see how to Navigate on Autopilot (Beta) for a Model 3, video by Tesla*

Now, there have been a few upgrades since 2015. [Hardware](https://en.wikipedia.org/wiki/Tesla_Autopilot#Hardware)


In April 2019, Tesla started releasing an update to Navigate on Autopilot, which does not require lane change confirmation, but does require the driver to have hands on the steering wheel. The car will navigate freeway interchanges on its own, but driver needs to supervise. The ability is available to those who have purchased Enhanced Autopilot or Full Self-Driving Capability.

> "Autopilot is an advanced driver assistance system that enhances safety and convenience behind the wheel. When used properly, Autopilot reduces your overall workload as a driver. 8 external cameras, a radar, 12 ultrasonic sensors and a powerful onboard computer provide an additional layer of safety to guide you on your journey."- [Tesla Support](https://www.tesla.com/support/autopilot)

#### **Full Self-Driving Capability**

*from Tesla Support*

- **Navigate on Autopilot (Beta):** Actively guides your car from a highwayâ€™s on-ramp to off-ramp, including suggesting lane changes, navigating interchanges, automatically engaging the turn signal and taking the correct exit
- **Auto Lane Change:** Assists in moving to an adjacent lane on the highway when Autosteer is engaged
Autopark: Helps automatically parallel or perpendicular park your car, with a single touch
- **Summon:** Moves your car in and out of a tight space using the mobile app or key
- **Smart Summon:** Your car will navigate more complex environments and parking spaces, maneuvering around objects as necessary to come find you in a parking lot.

For more Information on Full Self Driving Capabilities Visit:- [Tesla Support](https://www.tesla.com/support/autopilot)

In Problems ranging from perception to Control. [Wait for a few seconds for the video below to load...]

![alt text](https://1.bp.blogspot.com/-auwvUXsXvAM/XlDofLHhq4I/AAAAAAAAKaQ/nJUtfhJFZIAO4313ZbQB1ZkllGmclnF8wCNcBGAsYHQ/s1600/tt.gif)

*Video from:- [Tesla AutoPilotAI](https://www.tesla.com/autopilotAI)*

>"Our per-camera networks analyze raw images to perform semantic segmentation, object detection and monocular depth estimation. Our birds-eye-view networks take video from all cameras to output the road layout, static infrastructure and 3D objects directly in the top-down view. Our networks learn from the most complicated and diverse scenarios in the world, iteratively sourced from our fleet of nearly 1M vehicles in real time. A full build of Autopilot neural networks involves 48 networks that take 70,000 GPU hours to train ðŸ”¥. Together, they output 1,000 distinct tensors (predictions) at each timestep."- [Tesla AutoPilotAI](https://www.tesla.com/autopilotAI)

## **Credits/Citations**
- [Tesla Support](https://www.tesla.com/support/autopilot)
- [Lex Fridman Deep Learning State of the Art, 2020](https://www.youtube.com/watch?v=0VH1Lim8gL8)
- [ColdFusion, Tesla Autopilot](https://www.youtube.com/watch?v=Kt-rhVU8evI)
- [LaneNet: Real-Time Lane Detection Networks for Autonomous Driving](https://arxiv.org/pdf/1807.01726.pdf). [	arXiv:1807.01726 [cs.CV]](https://arxiv.org/abs/1807.01726)
- [End to End learning for Self Driving Cars](https://arxiv.org/pdf/1604.07316.pdf). [	arXiv:1604.07316 [cs.CV]](https://arxiv.org/abs/1604.07316)
- [Tesla AutoPilotAI](https://www.tesla.com/autopilotAI)
- [Tesla AutoPilot- Wikipedia](https://en.wikipedia.org/wiki/Tesla_Autopilot)
- [Medium Blog Post](https://medium.com/swlh/implementing-end-to-end-learning-for-self-driving-cars-251fd1635606)

