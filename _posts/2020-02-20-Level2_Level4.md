# (Continuing) Level 2 vs Level 4 Autonomous Driving System (Featuring:- Tesla,Waymo):- A Case Study and SOTA 2020.

1. TOC
{:toc}


## **A brief introduction to level 2 and level 4 Autonomous driving systems**

> *“The fundamental message consumers should be taking today is that it’s financially insane to buy anything other than a Tesla...
It’ll be like owning a horse in three years. I mean, fine if you want to own a horse, but you should go into it with that expectation.”- Elon Musk, Tesla Autonomy Day*

### **The Aim**

The aim through this blog post is to give the readers a brief insight into level 2 and level 4 autonomous driving systems.
We will try to gain perspective on what constitutes as level 2 and level 4. With the rise in Autopilot Systems by Tesla and Maps, Lidar  by Waymo, it is time to look into what these levels represent and try to generate a case study on the basis of open questions and the associated statistics.

A look into the hard engineering side of these two levels should definitely provide clarity. We will be discussing the state of the art for level 2 and level 4 in 2020 .

### **Levels of Driving Automation**


Lets talk about the classification levels first.
A classification system with six levels – ranging from fully manual to fully automated systems – 
was published in 2014 by SAE International. 

[*SAE International, previously known as the Society of Automotive Engineers, is a U.S.-based, globally active professional association and standards developing organization for engineering professionals in various industries. Principal emphasis is placed on global transport industries such as aerospace, automotive, and commercial vehicles.*]

This classification is based on the amount of driver intervention and attentiveness required, rather than the vehicle's capabilities, although these are loosely related.

In SAE's automation level definitions, "driving mode" means "a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)"

- Level 0: The automated system issues warnings and may momentarily intervene but has no sustained vehicle control.

- Level 1: ("hands on"): The driver and the automated system share control of the vehicle. Examples are systems where the driver controls steering and the automated system controls engine power to maintain a set speed (Cruise Control) or engine and brake power to maintain and vary speed (Adaptive Cruise Control or ACC); and Parking Assistance, where steering is automated while speed is under manual control. The driver must be ready to retake full control at any time. 

- Level 2: ("hands off"): The automated system takes full control of the vehicle: accelerating, braking, and steering. The driver must monitor the driving and be prepared to intervene immediately at any time if the automated system fails to respond properly. 

- Level 3: ("eyes off"): The driver can safely turn their attention away from the driving tasks, e.g. the driver can text or watch a movie. The vehicle will handle situations that call for an immediate response, like emergency braking. The driver must still be prepared to intervene within some limited time, when called upon by the vehicle to do so.

- Level 4: ("mind off"): As level 3, but no driver attention is ever required for safety, e.g. the driver may safely go to sleep or leave the driver's seat. Self-driving is supported only in limited spatial areas (geofenced) or under special circumstances. Outside of these areas or circumstances, the vehicle must be able to safely abort the trip, e.g. park the car, if the driver does not retake control.

- Level 5: ("steering wheel optional"): No human intervention is required at all. An example would be a robotic taxi.

*Source:- Wikipedia*

## **Level 2**
 
 As mentioned in the above definition, the automated system takes full control of the vehicle. But, the driver, still needs to be prepared to intervene if the system fails. 

One of the recent examples of a Level 2 Autonomous driving system comes from Tesla. Yes, you guessed it right the **Tesla Autopilot.** 

> "*Autopilot is a good thing to have in planes, and we should have it in cars."- Elon Musk, 2013.*

The Tesla Autopilot is an [advanced driver-assistance system](https://en.wikipedia.org/wiki/Advanced_driver-assistance_systems) feature offered by Tesla that has 

- **Lane centering/Auto Steer**- In road-transport terminology, lane centering, also known as auto steer, is a mechanism designed to keep a car centered in the lane, relieving the driver of the task of steering.

- **Adaptive cruise control**- Adaptive cruise control (ACC) is an available cruise control system for road vehicles that automatically adjusts the vehicle speed to maintain a safe distance from vehicles ahead .

- **Self-parking**- Automatic parking is an autonomous car-maneuvering system that moves a vehicle from a traffic lane into a parking spot to perform parallel, perpendicular, or angle parking. The automatic parking system aims to enhance the comfort and safety of driving in constrained environments where much attention and experience is required to steer the car.

In all of these features, the driver is responsible and the car requires constant supervision. 
As an upgrade to the base Autopilot capabilities, the company's stated intent is to offer **Full self-driving (FSD)** .
More on FSD Later on.


### **Deep Learning**

Level 2 takes control over acceleration, braking , steering. 
So, a part of the topic needs to cover the Deep Learning side as well as the Hardware side of the story
in order to look into how Level 2 can be acheived. We'll look into the Tesla Autopilot story a little later.
But first lets see how a level 2 state can be created and put into use. 

### **Using Computer Vision Techniques**

Let us assume for a moment, a scenario where Deep Learning is the cake to achieve Level 2.
In this case,  Deep Learning is used along with external hardware like UV sensors, cameras etc.
Neural Networks are trained and retrained using datasets [datasets consisting of different environments and edge cases] 
at scale, in some cases networks that improve over time (will be discussed later on) and deployed. 

This is the kind of Deep Learning that is supervised as it easy to collect data and train the network.
There is a good chance that this information will be of the highest resolution.
The cons of such a system is that it will require a huge amount of data and is not explainable enough. 
Let us look at a few examples:-

### **Examples**

#### **Paper 1:- [End to End learning for Self Driving Cars](https://arxiv.org/pdf/1604.07316.pdf)**

In 2016, researchers from NVIDIA trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved to be powerful. In the [paper](https://arxiv.org/pdf/1604.07316.pdf), the authors were able to demonstrate that CNNs were able to learn the entire task of lane and road following without manual decomposition into road or lane marking detection, path planning, and control. It learns the entire processing pipeline needed to steer an automobile.

>"*A small amount of training data from less than a hundred hours of driving was sufficient to train the car to operate in diverse conditions, on highways, local and residential
roads in sunny, cloudy, and rainy conditions. The CNN is able to learn meaningful road features from a very sparse training signal (steering alone)"- Conclusion from End to End Learning for Self-Driving Cars.*

#### **Methodology**

A Defense Advanced Research Projects Agency (DARPA) seedling project known as DARPA Autonomous Vehicle (DAVE) was used. DAVE-2, the system that was built in the paper on top of DAVE.

To get the training data for DAVE-2 ,a data acquisition car was used. The car was equipped with three cameras mounted behind the windshield of the car. Video was captured simultaneously with the steering angle data.  

Images were fed into a CNN which
then calculated a proposed NN steering command. 
The proposed command was then compared to the desired command for that image and the weights of the CNN
were adjusted to bring the CNN output closer to
the desired output. The weight adjustment is accomplished using back propagation. 

![alt text](https://miro.medium.com/max/1184/1*bO9_3Vp5InMkdJ3_5b4D1Q.png)

*A block diagram of the training system. Image from the End to End Learning for Self driving cars paper.*


**Dataset**- Most road data was collected in central New Jersey,  highway data was also collected from Illinois, Michigan, Pennsylvania, and New York. 

**Credits**
- End to End Learning for Self Driving Cars paper
- [Medium Blog Post](https://medium.com/swlh/implementing-end-to-end-learning-for-self-driving-cars-251fd1635606)


#### **Paper 2:- [LaneNet: Real-Time Lane Detection Networks for Autonomous Driving](https://arxiv.org/pdf/1807.01726.pdf)**

[ZE Wang et.al](https://arxiv.org/pdf/1807.01726.pdf). When we talk about applying in autonomous driving projects, lane detection is one of the most preliminary project ideas that come to mind for a self driving car enthusiast. Since, we are considering Vision techiques for a level 2 system, we are going to take a look into methods that can provide some perspective where Deep Learning is the cake.In this example, we are going to look into LaneNet. 

#### **Methodology**

The authors of the paper has proposed a deep neural network based method called LaneNet, in order to break down the lane detection into two stages:-

> "lane edge proposal and lane line localization. Stage one uses a lane edge proposal network for
pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based
on lane edge proposals....Despite all the difficulties, our lane detection is shown to be
robust to both highway and urban road scenarios method
without relying on any assumptions on the lane number or
the lane line patterns. The high running speed and low computational cost endow our LaneNet the capability of being
deployed on vehicle-based systems. Experiments validate
that our LaneNet consistently delivers outstanding performances on real world traffic scenarios.
"- from the Abstract from [ZE Wang et.al](https://arxiv.org/pdf/1807.01726.pdf)


In order to work towards a more effective generalized, low computation cost, and real-time vehicle-based solution, the authors have proposed LaneNet, a deep Neural network based system that breaks down lane detection into two parts:- Lane edge proposal and lane Localization.

> " In the lane edge proposal stage, the proposal network runs binary classification on every pixel of an input
image for generating lane edge proposals, which are served
as the input to the lane line localization network in the second stage."- from the Introduction from [ZE Wang et.al](https://arxiv.org/pdf/1807.01726.pdf)

The first  network detects the
edges of lane marks, and generates a pixel-wise lane edge proposal map. The second localization network determines
the localization and shape of each lane based on lane edge
proposals. The authors have adopted a light weight Encoder Decoder architecture that uses convolutional layers and deconv layers.

> "The encoder takes an IPM image of the front
view of a vehicle as the input, and hierarchically extracts
the features. The decoder progressively recovers the resolution of the feature map and produce a pixel-wise lane edge
proposal map."- from the paper section 2

*IPM - Inverse Perspective Mapping-  The Inverse Perspective Mapping (IPM) The angle of view under which a scene is acquired and the distance of the objects from the camera (namely the perspective effect) contribute to associate a different information content to each pixel of an image.*

![alt_text](images/Screenshot%20(293).png)

*Image from the paper*

The line localization network also uses an encoding-decoder
structure. It takes the coordinates of the lane edges as input,
and applies a series of 1D convolution and pooling operations
to encode an input to a low dimensional representation. An (LSTM) based
decoder is used to  decode the representation of each lane in the image. 

![alt_text](images/Screenshot%20(290).png)

*Image from the paper*

**Training and Testing**- In the paper, the model takes an image of
the front view of a vehicle as input, and outputs a lane edge
probability map of the same size as the input image. It happens in a supervised manner. LaneNet has been tested on real world traffic data. The authors had compared their method with other lane detection
methods. The dataset consists of more than 5000 annotated front view images taken on both highways and urban roads. 

![alt text](https://d3i71xaburhd42.cloudfront.net/52a8866dfd2bce6a1169eba1a47ad2008ef4eecd/7-Figure4-1.png)

*The original images along with the corresponding lane edge proposal maps and final detection results- Image from the paper*

LaneNet can be applied to diverse situations. Since Deep Learning here is the cake, a high robustness is expected. The two stage detection pipeline put forth by the authors, greatly reduces computational cost. 

**Other Papers**- 
- [Real time Lane Detection for Autonomous Vehicles](https://ieeexplore.ieee.org/document/4580573)
- [Towards End-to-End Lane Detection: an Instance Segmentation
Approach](https://arxiv.org/pdf/1802.05591v1.pdf)
- [Lane Detection in Low-light Conditions Using an Efficient Data
Enhancement : Light Conditions Style Transfer](https://arxiv.org/pdf/2002.01177.pdf)

**Github Repositories**
- [Robust Lane Detection and Tracking](https://github.com/ayush1997/Robust-Lane-Detection-and-Tracking)

> - Updated:- Feb 20,2020.
> - Updated :- Paper 2, Tesla Autopilot, Feb 21,2020.
