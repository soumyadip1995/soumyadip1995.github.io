# Level 2 vs Level 4 Autonomous Driving System:- A Case Study (Continuing)

> *“The fundamental message consumers should be taking today is that it’s financially insane to buy anything other than a Tesla...
It’ll be like owning a horse in three years. I mean, fine if you want to own a horse, but you should go into it with that expectation.”- Elon Musk, Tesla Autonomy Day*

## **A brief introduction to level 2 and level 4 Autonomous driving systems**
### **Aim**

The aim through this blog post is to give the readers a brief insight into level 2 and level 4 autonomous driving systems.
We will try to gain perspective on what constitutes as level 2 and level 4 and the work that has been done so far on these
two levels by the likes of companies like Tesla and Waymo. 

A look into behind the scenes should definitely provide clarity. We will be discussing the state of the art 
level 2 and level 4 in 2020 and try to answer some of the questions that one might have.

### **Levels of Driving Automation**


Lets talk about the classification levels first.
A classification system with six levels – ranging from fully manual to fully automated systems – 
was published in 2014 by SAE International. 

[*SAE International, previously known as the Society of Automotive Engineers, is a U.S.-based, globally active professional association and standards developing organization for engineering professionals in various industries. Principal emphasis is placed on global transport industries such as aerospace, automotive, and commercial vehicles.*]

This classification is based on the amount of driver intervention and attentiveness required, rather than the vehicle's capabilities, although these are loosely related.

In SAE's automation level definitions, "driving mode" means "a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)"

- Level 0: The automated system issues warnings and may momentarily intervene but has no sustained vehicle control.

- Level 1: ("hands on"): The driver and the automated system share control of the vehicle. Examples are systems where the driver controls steering and the automated system controls engine power to maintain a set speed (Cruise Control) or engine and brake power to maintain and vary speed (Adaptive Cruise Control or ACC); and Parking Assistance, where steering is automated while speed is under manual control. The driver must be ready to retake full control at any time. 

- Level 2: ("hands off"): The automated system takes full control of the vehicle: accelerating, braking, and steering. The driver must monitor the driving and be prepared to intervene immediately at any time if the automated system fails to respond properly. 

- Level 3: ("eyes off"): The driver can safely turn their attention away from the driving tasks, e.g. the driver can text or watch a movie. The vehicle will handle situations that call for an immediate response, like emergency braking. The driver must still be prepared to intervene within some limited time, when called upon by the vehicle to do so.

- Level 4: ("mind off"): As level 3, but no driver attention is ever required for safety, e.g. the driver may safely go to sleep or leave the driver's seat. Self-driving is supported only in limited spatial areas (geofenced) or under special circumstances. Outside of these areas or circumstances, the vehicle must be able to safely abort the trip, e.g. park the car, if the driver does not retake control.

- Level 5: ("steering wheel optional"): No human intervention is required at all. An example would be a robotic taxi.

*Source:- Wikipedia*

## **Level 2**
 
 As mentioned in the above definition, the automated system takes full control of the vehicle. But, the driver, still needs to be prepared to intervene if the system fails. 

One of the recent examples of a Level 2 Autonomous driving system comes from Tesla. Yes, you guessed it right the **Tesla Autopilot.** 

> "*Autopilot is a good thing to have in planes, and we should have it in cars."- Elon Musk, 2013.*

The Tesla Autopilot is an [advanced driver-assistance system](https://en.wikipedia.org/wiki/Advanced_driver-assistance_systems) feature offered by Tesla that has 

- **Lane centering/Auto Steer**- In road-transport terminology, lane centering, also known as auto steer, is a mechanism designed to keep a car centered in the lane, relieving the driver of the task of steering.

- **Adaptive cruise control**- Adaptive cruise control (ACC) is an available cruise control system for road vehicles that automatically adjusts the vehicle speed to maintain a safe distance from vehicles ahead .

- **Self-parking**- Automatic parking is an autonomous car-maneuvering system that moves a vehicle from a traffic lane into a parking spot to perform parallel, perpendicular, or angle parking. The automatic parking system aims to enhance the comfort and safety of driving in constrained environments where much attention and experience is required to steer the car.

In all of these features, the driver is responsible and the car requires constant supervision. 
As an upgrade to the base Autopilot capabilities, the company's stated intent is to offer **Full self-driving (FSD)** .
More on FSD Later on.


### **Deep Learning**

Level 2 takes control over acceleration, braking , steering. 
So, a part of the topic needs to cover the Deep Learning side as well as the Hardware side of the story
in order to look into how Level 2 can be acheived. We'll look into the Tesla Autopilot story a little later.
But first lets see how a level 2 state can be created and put into use. 

### **Using Computer Vision Techniques**

Let us assume for a moment, a scenario where Deep Learning is the cake to achieve Level 2.
In this case,  Deep Learning is used along with external hardware like UV sensors, cameras etc.
Neural Networks are trained and retrained using datasets [datasets consisting of different environments and edge cases] 
at scale, in some cases networks that improve over time (will be discussed later on) and deployed. 

This is the kind of Deep Learning that is supervised as it easy to collect data and train the network.
There is a good chance that this information will be of the highest resolution.
The cons of such a system is that it will require a huge amount of data and is not explainable enough. 
Let us look at a few examples:-

### **Examples**

#### **Paper 1:- [End to End learning for Self Driving Cars](https://arxiv.org/pdf/1604.07316.pdf)**

In 2016, researchers from NVIDIA trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved to be powerful. In the [paper](https://arxiv.org/pdf/1604.07316.pdf), the authors were able to demonstrate that CNNs were able to learn the entire task of lane and road following without manual decomposition into road or lane marking detection, path planning, and control. It learns the entire processing pipeline needed to steer an automobile.

>"*A small amount of training data from less than a hundred hours of driving was sufficient to train the car to operate in diverse conditions, on highways, local and residential
roads in sunny, cloudy, and rainy conditions. The CNN is able to learn meaningful road features from a very sparse training signal (steering alone)"- Conclusion from End to End Learning for Self-Driving Cars.*

#### **Methodology**

A Defense Advanced Research Projects Agency (DARPA) seedling project known as DARPA Autonomous Vehicle (DAVE) was used. DAVE-2, the system that was built in the paper on top of DAVE.

To get the training data for DAVE-2 ,a data acquisition car was used. The car was equipped with three cameras mounted behind the windshield of the car. Video was captured simultaneously with the steering angle data.  

Images were fed into a CNN which
then calculated a proposed NN steering command. 
The proposed command was then compared to the desired command for that image and the weights of the CNN
were adjusted to bring the CNN output closer to
the desired output. The weight adjustment is accomplished using back propagation. 

![alt text](https://miro.medium.com/max/1184/1*bO9_3Vp5InMkdJ3_5b4D1Q.png)

*A block diagram of the training system. Image from the End to End Learning for Self driving cars paper.*


**Dataset**- Most road data was collected in central New Jersey,  highway data was also collected from Illinois, Michigan, Pennsylvania, and New York. 

**Credits**
- End to End Learning for Self Driving Cars paper
- [Medium Blog Post](https://medium.com/swlh/implementing-end-to-end-learning-for-self-driving-cars-251fd1635606)


#### **Paper 2:- [LaneNet: Real-Time Lane Detection Networks for Autonomous Driving](https://arxiv.org/pdf/1807.01726.pdf)**
